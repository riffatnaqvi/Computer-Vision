{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "342d6c69"
      },
      "source": [
        "# Task\n",
        "Extract individual payment slips from the image `/content/Slip.jpeg` using instance segmentation and save each extracted slip as a separate image file in a newly created directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f557c51"
      },
      "source": [
        "## Load Image and Deep Learning Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fa9fe93",
        "outputId": "35472209-94a7-4d08-a400-fa645a9e3da5"
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "\n",
        "# 1. Load the image\n",
        "image_path = '/content/Slip.jpeg'\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "print(f\"Image '{image_path}' loaded successfully.\")\n",
        "\n",
        "# 2. and 3. Import and instantiate a pre-trained Mask R-CNN model\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "print(\"Mask R-CNN model loaded with pre-trained weights.\")\n",
        "\n",
        "# 4. Set the model to evaluation mode\n",
        "model.eval()\n",
        "print(\"Model set to evaluation mode.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image '/content/Slip.jpeg' loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask R-CNN model loaded with pre-trained weights.\n",
            "Model set to evaluation mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c22e2f03",
        "outputId": "56e6b61f-0bfa-4049-fece-c307ea455f7a"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 1. Define a transformation pipeline\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "# 2. Apply the transformation to the loaded image\n",
        "img_tensor = transform(image)\n",
        "\n",
        "# 3. Perform inference by passing the transformed image to the model\n",
        "# The model expects a list of tensors as input\n",
        "with torch.no_grad():\n",
        "    prediction = model([img_tensor])\n",
        "\n",
        "# 4. Store the predictions\n",
        "# For Mask R-CNN, prediction[0] contains dictionaries with 'boxes', 'labels', 'scores', and 'masks'\n",
        "print(f\"Inference completed. Found {len(prediction[0]['labels'])} potential objects.\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference completed. Found 5 potential objects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "509b37ff",
        "outputId": "682dbc65-c73d-4bfd-b167-ada010510d6b"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "\n",
        "# 1. Define a confidence threshold for filtering predictions\n",
        "score_threshold = 0.1 # Lowering the threshold to capture more objects\n",
        "\n",
        "# Filter predictions based on confidence score\n",
        "masks = prediction[0]['masks'][prediction[0]['scores'] > score_threshold]\n",
        "boxes = prediction[0]['boxes'][prediction[0]['scores'] > score_threshold]\n",
        "labels = prediction[0]['labels'][prediction[0]['scores'] > score_threshold]\n",
        "scores = prediction[0]['scores'][prediction[0]['scores'] > score_threshold]\n",
        "\n",
        "print(f\"Found {len(boxes)} objects above confidence threshold {score_threshold}.\")\n",
        "\n",
        "# Create a directory to save the extracted slips\n",
        "output_dir = \"payment_slips_extracted\" # Changed folder name\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Output directory '{output_dir}' created or already exists.\")\n",
        "\n",
        "# Process each detected object\n",
        "for i in range(len(boxes)):\n",
        "    # Get the mask for the current object\n",
        "    # The mask is typically a float tensor, convert to boolean and then to numpy\n",
        "    mask = (masks[i] > 0.5).squeeze().cpu().numpy()\n",
        "\n",
        "    # Get the bounding box coordinates\n",
        "    # Convert tensor to list of integers\n",
        "    box = [int(b) for b in boxes[i].tolist()]\n",
        "    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
        "\n",
        "    # Convert the original PIL image to a numpy array for masking and cropping\n",
        "    img_np = np.array(image)\n",
        "\n",
        "    # Apply the mask to the original image to get the segmented object\n",
        "    # Ensure mask has the same dimensions as the image slice\n",
        "    cropped_mask = mask[y1:y2, x1:x2]\n",
        "    # Resize mask if it does not match the image slice dimensions exactly due to rounding\n",
        "    if cropped_mask.shape != (y2 - y1, x2 - x1):\n",
        "        cropped_mask = resize(cropped_mask, (y2 - y1, x2 - x1), anti_aliasing=True) > 0.5\n",
        "    segmented_image_data = np.zeros((y2 - y1, x2 - x1, 4), dtype=np.uint8)\n",
        "\n",
        "    # Extract the region of interest from the original image\n",
        "    roi = img_np[y1:y2, x1:x2]\n",
        "\n",
        "    # Apply the mask to the ROI for RGB channels\n",
        "    # Only pixels where cropped_mask is True will be copied\n",
        "    segmented_image_data[cropped_mask, :3] = roi[cropped_mask]\n",
        "\n",
        "    # Set the alpha channel to 255 (opaque) where the mask is True\n",
        "    segmented_image_data[cropped_mask, 3] = 255\n",
        "\n",
        "    # Create PIL Image from numpy array\n",
        "    extracted_slip = Image.fromarray(segmented_image_data)\n",
        "\n",
        "    # Save the extracted slip\n",
        "    output_filename = os.path.join(output_dir, f\"slip_{i+1}.png\")\n",
        "    extracted_slip.save(output_filename)\n",
        "    print(f\"Extracted slip {i+1} saved to {output_filename}\")\n",
        "\n",
        "print(\"All relevant payment slips extracted and saved.\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 objects above confidence threshold 0.1.\n",
            "Output directory 'payment_slips_extracted' created or already exists.\n",
            "Extracted slip 1 saved to payment_slips_extracted/slip_1.png\n",
            "Extracted slip 2 saved to payment_slips_extracted/slip_2.png\n",
            "Extracted slip 3 saved to payment_slips_extracted/slip_3.png\n",
            "All relevant payment slips extracted and saved.\n"
          ]
        }
      ]
    }
  ]
}